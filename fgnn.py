# -*- coding: utf-8 -*-
"""FGNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KLngsawQWbEx-lqyR3AIpSIjKG7tAvS
"""

!nvidia-smi

# PyTorch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# For data preprocess
import numpy as np
import csv
import os
import pickle
# For plotting
import time
import torch.nn.functional as F
import random
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
from tqdm.auto import tqdm
from sklearn.preprocessing import MinMaxScaler 

myseed = 42069  # set a random seed for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(myseed)
torch.manual_seed(myseed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(myseed)

device = "cuda" if torch.cuda.is_available() else "cpu"

def single_minmaxscale(data, scale_range):
    def minmaxscale(data, scale_range):
        scaler = MinMaxScaler(scale_range)
        scaler.fit(data)
        normalized = scaler.transform(data)
        return normalized

    X = []
    for i in data:
        X.append(minmaxscale(i.reshape(-1,1), scale_range))
    return np.asarray(X)     


def data_preproc(dataset, shot = 0, scale_range = (-1, 1)):
    X_tra, y_tra, X_tst, y_tst = dataset
    X_tra = single_minmaxscale(X_tra, scale_range)
    X_tst = single_minmaxscale(X_tst, scale_range)

    X_tra = X_tra.astype('float32')
    X_tra = X_tra.reshape(-1,1,120)
    X_tst = X_tst.astype('float32')
    X_tst = X_tst.reshape(-1,1,120)
    data = np.concatenate((X_tra,X_tst))
    label = np.concatenate((y_tra,y_tst))
    # print('Finished preprocessing.')
    
    return data, label
    

def euclidean_metric(a, b):
    n = a.shape[0]
    m = b.shape[0]
    a = a.unsqueeze(1).expand(n, m, -1)
    b = b.unsqueeze(0).expand(n, m, -1)
    logits = -((a - b)**2).sum(dim=2)
    return logits

def count_acc(logits, label):
    pred = torch.argmax(F.softmax(logits), dim=1)
    return (pred == label).type(torch.cuda.FloatTensor).mean().item()

def label2edge(label):
    ns = label.shape[1]
    label = label.unsqueeze(-1).repeat(1, 1, ns)
    edge = torch.eq(label, label.transpose(1,2)).float()
    return edge

target_path = './drive/MyDrive/CSI_data/EXP3.pickle'
source_path = './drive/MyDrive/CSI_data/EXP1.pickle'
model_path = './drive/MyDrive/CSI_data/model_fine_tuning.ckpt'

nway = 18
nshot = 5
batch_size = 16
n_iters = 6000
log_interval = 100
log_eval = 1000
train_type = 'transduction'
# train_type = 'induction'

lr = 0.01
alpha = None
beta = 1

reg = 0.1

class MyDataset(Dataset):
  def __init__(self,data,label=None):
    super(MyDataset,self).__init__()
    self.data = data
    self.label = label
  def __getitem__(self,idx):
    data = self.data[idx]
    if self.label is not None:
      label = self.label[idx]
      return data, label
    else:
      return data, 1
  def __len__(self):
    return len(self.data)
def count_data(data_dict):
  num = 0
  for key in data_dict.keys():
      num += len(data_dict[key])
  return num

class MyDataLoader(Dataset):
  def __init__(self,source_path,target_path,train=True,nway=5):
    super(MyDataLoader,self).__init__()
    self.nway = nway
    self.input_channel = 1
    self.size = 120
    self.full_data_dict, self.few_data_dict = self.load_data(source_path,target_path,train)

    print('full_data_num: %d' % count_data(self.full_data_dict))
    print('few_data_num: %d' % count_data(self.few_data_dict))
    
    
  def load_data(self,source_path,target_path,train):
    full_data_dict = {}
    few_data_dict = {}
    data_full, label_full = data_preproc(np.asarray(pickle.load(open(source_path,'rb'))),shot=0)
    data_few, label_few = data_preproc(np.asarray(pickle.load(open(target_path,'rb'))),shot=5)
    if train_type == 'transduction':
      data_full = np.concatenate((data_full,data_few))
      temp_label = np.copy(label_few) + 16
      label_full = np.concatenate((label_full,temp_label))
    for i in range(len(label_full)):
      if label_full[i] not in full_data_dict:
        full_data_dict[label_full[i]] = [data_full[i]]
      else:
        full_data_dict[label_full[i]].append(data_full[i])

    for i in range(len(label_few)):
      if label_few[i] not in few_data_dict:
        few_data_dict[label_few[i]] = [data_few[i]]
      else:
        few_data_dict[label_few[i]].append(data_few[i])
    if train_type == 'transduction':
      for i in range(16,34):
        full_data_dict[i] = full_data_dict[i][:nshot+1]
    return full_data_dict, few_data_dict

  def load_batch_data(self,train=True,batch_size=16,nway=5,num_shot=5):
    if train:
      data_dict = self.full_data_dict
    else:
      data_dict = self.few_data_dict
    
    x = []
    label_y = []
    one_hot_y = []
    class_y = []

    xi = []
    label_yi = []
    one_hot_yi = []

    map_label2class = []

    for i in range(batch_size):
      sampled_classes = random.sample(data_dict.keys(),nway)
      positive_class = random.randint(0,nway-1)
      label2class = torch.LongTensor(nway)

      single_xi = []
      single_one_hot_yi = []
      single_label_yi = []
      single_class_yi = []

      for j, n_class in enumerate(sampled_classes):
        if j == positive_class:
          sampled_data = random.sample(data_dict[n_class],num_shot+1)
          x.append(torch.from_numpy(sampled_data[0]))
          label_y.append(torch.LongTensor([j]))
          one_hot = torch.zeros(nway)
          one_hot[j] = 1.0
          one_hot_y.append(one_hot)
          class_y.append(torch.LongTensor([n_class]))
          shots_data = torch.Tensor(sampled_data[1:])
        else:
          shots_data = torch.Tensor(random.sample(data_dict[n_class],num_shot))
        
        single_xi += shots_data
        single_label_yi.append(torch.LongTensor([j]).repeat(num_shot))
        one_hot = torch.zeros(nway)
        one_hot[j] = 1.0
        single_one_hot_yi.append(one_hot.repeat(num_shot,1))

        label2class[j] = n_class
      
      shuffle_index = torch.randperm(num_shot*nway)
      xi.append(torch.stack(single_xi,0)[shuffle_index])
      label_yi.append(torch.cat(single_label_yi,dim=0)[shuffle_index])
      one_hot_yi.append(torch.cat(single_one_hot_yi,dim=0)[shuffle_index])
      map_label2class.append(label2class)

    return [torch.stack(x,0), torch.cat(label_y,dim=0), torch.stack(one_hot_y,0), torch.cat(class_y,dim=0), torch.stack(xi,0), torch.stack(label_yi,0), torch.stack(one_hot_yi,0), torch.stack(map_label2class,0)]
  
  def load_tr_batch(self,batch_size=16,nway=5,num_shot=5):
    return self.load_batch_data(True,batch_size,nway,num_shot)
  def load_te_batch(self,batch_size=16,nway=5,num_shot=5):
    return self.load_batch_data(False,batch_size,nway,num_shot)
  
  def get_data_list(self, data_dict):
    data_list = []
    label_list = []
    for i in data_dict.keys():
      for data in data_dict[i]:
        data_list.append(data)
        label_list.append(i)

    now_time = time.time()

    random.Random(now_time).shuffle(data_list)
    random.Random(now_time).shuffle(label_list)

    return data_list, label_list

  def get_full_data_dict(self):
    return self.get_data_list(self.full_data_dict)
  def get_few_data_dict(self):
    return self.get_data_list(self.few_data_dict)

class Identity(nn.Module):
  def __init__(self):
    super(Identity, self).__init__()
      
  def forward(self, x):
    return x

class pretrain(nn.Module):
  def __init__(self,in_dim=120,label_num=18):
    super(pretrain,self).__init__()
    self.in_dim = in_dim
    self.ch = 32
    self.out = label_num
    self.C = nn.Sequential(
        nn.Conv1d(1,self.ch,5,2,2),
        nn.BatchNorm1d(self.ch),
        nn.ReLU(),
        nn.Conv1d(self.ch,self.ch,5,2,2),
        nn.BatchNorm1d(self.ch),
        nn.ReLU(),
        nn.Conv1d(self.ch,self.ch,5,2,2),
        nn.BatchNorm1d(self.ch),
        nn.ReLU(),
        nn.Flatten(),
        nn.Linear(self.ch*15,32),
        nn.BatchNorm1d(32),
        nn.ReLU(),
        nn.Linear(32,self.out)
    )
  def forward(self,x):
    return self.C(x)

class Gconv(nn.Module):
  def __init__(self,in_dim,out_dim,use_bn=True):
    super(Gconv,self).__init__()
    self.weight = nn.Linear(in_dim,out_dim)
    if use_bn:
      self.bn = nn.BatchNorm1d(out_dim)
    else:
      self.bn = None
  def forward(self,x,A):
    x_next = torch.matmul(A,x)
    x_next = self.weight(x_next)

    if self.bn is not None:
      x_next = torch.transpose(x_next,1,2)
      x_next = x_next.contiguous()
      x_next = self.bn(x_next)
      x_next = torch.transpose(x_next,1,2)
    return x_next

class Adj_layer(nn.Module):
  def __init__(self,in_dim,hidden_dim,ratio=[2,2,1,1]):
    super(Adj_layer,self).__init__()
    module_list = []
    inner_list = []
    for i in range(len(ratio)):
      if i==0:
        module_list.append(nn.Conv2d(in_dim,hidden_dim*ratio[i],1,1))
      else:
        module_list.append(nn.Conv2d(hidden_dim*ratio[i-1],hidden_dim*ratio[i],1,1))
      module_list.append(nn.BatchNorm2d(hidden_dim*ratio[i]))
      module_list.append(nn.LeakyReLU())
    module_list.append(nn.Conv2d(hidden_dim*ratio[-1],1,1,1))

    # inner_list.append(nn.Conv2d(1,4,1,1))
    # inner_list.append(nn.BatchNorm2d(4))
    # inner_list.append(nn.LeakyReLU())
    inner_list.append(nn.Conv2d(1,1,1,1))

    self.module_list = nn.ModuleList(module_list)
    self.inner_list = nn.ModuleList(inner_list)

  def forward(self,x):
    V = x.shape[1]
    x_i = x.unsqueeze(2)
    x_j = torch.transpose(x_i,1,2)

    phi = torch.abs(x_i-x_j)
    phi = torch.transpose(phi,1,3)

    # dot = torch.sum((x_i*x_j),dim=3)
    # dot = dot.unsqueeze(1)

    A = phi
    for l in self.module_list:
      A = l(A)
    
    A = torch.transpose(A,1,3)
    A = F.softmax(A,2).squeeze(3)

    # B = dot
    
    # for l in self.inner_list:
    #   B = l(B)

    # B = torch.transpose(B,1,3)
    # B = F.softmax(B,2)

    # A = (F.sigmoid(A)+F.sigmoid(B))/2
    
    if beta != 1:
      index = torch.topk(A,int(beta*V),dim=2)[1]
      mask_A = torch.zeros(A.shape).to(device)
      
      for i, ind in enumerate(index):

        col = torch.arange(nway*nshot+1).reshape(-1,1).expand(nway*nshot+1,int(beta*V)).reshape(-1)
        ind = ind.reshape(-1)
        mask_A[i][col,ind] = A[i][col,ind]
      
      return mask_A

    return A

class GNN_module(nn.Module):
  def __init__(self,nway,in_dim,hidden_dim,num_layers,feature='dense'):
    super(GNN_module,self).__init__()
    self.feature_type = feature
    adj_list = []
    GNN_list = []

    ratio = [2,1]

    if self.feature_type == 'dense':
      for i in range(num_layers):
        adj_list.append(Adj_layer(in_dim+hidden_dim//2*i,hidden_dim,ratio))
        GNN_list.append(Gconv(in_dim+hidden_dim//2*i,hidden_dim//2))

      last_adj = Adj_layer(in_dim+hidden_dim//2*num_layers,hidden_dim,ratio)
      last_conv = Gconv(in_dim+hidden_dim//2*num_layers,nway,use_bn=False)

    self.adj_list = nn.ModuleList(adj_list)
    self.GNN_list = nn.ModuleList(GNN_list)
    self.last_adj = last_adj
    self.last_conv = last_conv

  def forward(self,x):
    for i, _ in enumerate(self.adj_list):
      adj_layer = self.adj_list[i]
      conv_block = self.GNN_list[i]

      A = adj_layer(x)
      x_next = conv_block(x,A)
      x_next = F.leaky_relu(x_next,0.1)

      x = torch.cat([x,x_next],dim=2)

    A = self.last_adj(x)
    out = self.last_conv(x,A)

    return out[:,0,:], A

class GNN(nn.Module):
  def __init__(self,cnn_feature_size,gnn_feature_size,nway):
    super(GNN,self).__init__()
    num_inputs = cnn_feature_size + nway
    gnn_layer = 2
    self.gnn = GNN_module(nway,num_inputs,gnn_feature_size,gnn_layer,'dense')

  def forward(self,inputs):
    logits, A = self.gnn(inputs)
    logits = logits.squeeze(-1)

    return logits, A

class gnnModel(nn.Module):
  def __init__(self,nway):
    super(gnnModel,self).__init__()
    cnn_feature_size = 32
    gnn_feature_size = 16
    self.cnn_feature = pretrain()
    self.gnn = GNN(cnn_feature_size,gnn_feature_size,nway)
    self.fusion = nn.Conv2d(2,1,1,1)

  def forward(self,data):
    [x,_,_,_,xi,_,one_hot_yi,_] = data
    
    z = self.cnn_feature(x)
    zi_s = [self.cnn_feature(xi[:,i,:,:]) for i in range(xi.size(1))]
    zi_s = torch.stack(zi_s,dim=1)
    
    uniform_pad = torch.FloatTensor(one_hot_yi.size(0),1,one_hot_yi.size(2)).fill_(1.0/one_hot_yi.size(2)).to(device)
    labels = torch.cat([uniform_pad,one_hot_yi],dim=1)
    features = torch.cat([z.unsqueeze(1),zi_s],dim=1)
    
    if alpha != None:
      C_x = F.softmax(torch.matmul(features,torch.transpose(features,1,2)),dim=1)
      C_y = torch.matmul(labels,torch.transpose(labels,1,2))
      C_x = C_x.unsqueeze(1)
      C_y = C_y.unsqueeze(1)
      C_f = self.fusion(torch.cat([C_x,C_y],dim=1)).squeeze()
      
      X1 = torch.matmul(C_f,features)
      Y1 = alpha*labels + (1-alpha)*torch.matmul(C_f,labels)

      nodes_features = torch.cat([X1,Y1],dim=2)
    else:
      nodes_features = torch.cat([features,labels],dim=2)

    out_logits, A = self.gnn(nodes_features)
    logits_prob = F.log_softmax(out_logits,dim=1)

    return logits_prob, A

class Trainer():
  def __init__(self):

    self.tr_dataloader = MyDataLoader(source_path,target_path,True,nway)
    self.model = gnnModel(nway)
    self.iter = 0
    self.sample_size = 32

  def load_model(self,model_path):
    self.model.load_state_dict(torch.load(model_path))

  def load_pretrain(self,model_path):
    self.model.cnn_feature.load_state_dict(torch.load(model_path))
    self.model.cnn_feature.C[13] = Identity()

  def train_batch(self):
    self.model.train()
    data = self.tr_dataloader.load_tr_batch(batch_size=batch_size,nway=nway,num_shot=nshot)
    data = [(_data).to(device) for _data in data]
    self.opt.zero_grad()
    logsoft_prob, A = self.model(data)
    label = data[1]
    loss = F.nll_loss(logsoft_prob,label)
    pred = torch.argmax(logsoft_prob,dim=1)
    acc = (pred == label).type(torch.cuda.FloatTensor).mean().item()

    label = torch.cat([data[1].unsqueeze(1),data[5]],dim=1)
    label_edge = label2edge(label)
    # label_edge = F.normalize(label_edge,p=2,dim=2)
    loss_e = F.binary_cross_entropy(A,label_edge)
    # print(reg*loss_e.item())
    loss += reg * loss_e

    loss.backward()
    self.opt.step()

    return loss.item(), acc

  def train(self):
    for p in self.model.cnn_feature.parameters():
      p.requires_grad = True
    self.model.to(device)
    self.opt = torch.optim.Adam(filter(lambda p:p.requires_grad,self.model.parameters()),lr=lr,weight_decay=1e-6)
    # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt,T_max=100,eta_min=0.0001,last_epoch=-1)
    best_loss = 1e8
    best_acc = 0
    eval_sample = 6400
    train_loss = []
    train_acc = []
    stop = 0
    for i in range(n_iters):
      loss, acc = self.train_batch()
      train_loss.append(loss)
      train_acc.append(acc)
      if i%log_interval==0:
        print(f"[Train | {i}/{n_iters} ] loss = {np.mean(train_loss):.5f}, acc = {np.mean(train_acc)*100:.5f} %")
        train_loss = []
        train_acc = []
        
      if (i+1)%log_eval==0:
        val_loss, val_acc = self.eval(self.tr_dataloader,eval_sample)
        print(f"[Val | {i+1}/{n_iters} ] loss = {val_loss:.5f}, acc = {val_acc:.5f} %")

        if val_acc > best_acc:
          stop = 0
          best_loss = val_loss
          best_acc = val_acc

        stop += 1 

      # self.scheduler.step()

    print(f"[Best Result | for {n_iters} iterations ] best loss = {best_loss:.5f}, best_acc = {best_acc:.5f} %")


  def eval(self,dataloader,test_sample):
    self.model.eval()
    iteration = int(test_sample/batch_size)

    total_loss = 0.0
    total_sample = 0
    total_acc = 0
    with torch.no_grad():
      for i in range(iteration):
        data = dataloader.load_te_batch(batch_size,nway,nshot)
        data = [(_data).to(device) for _data in data]
        logsoft_prob, A = self.model(data)
        label = data[1]
        loss = F.nll_loss(logsoft_prob,label)
        total_loss += loss.item() * logsoft_prob.shape[0]
        pred = torch.argmax(logsoft_prob,dim=1)

        total_acc += torch.eq(pred,label).float().sum().item()
        total_sample += pred.shape[0]

        label = torch.cat([data[1].unsqueeze(1),data[5]],dim=1)
        label_edge = label2edge(label)
        label_edge = F.normalize(label_edge,p=2,dim=2)

        loss_e = F.binary_cross_entropy_with_logits(A,label_edge)
        total_loss += reg * loss_e

    return total_loss/total_sample, total_acc/total_sample*100

  def test(self, test_data_array, te_dataloader):
    self.model.to(device)
    self.model.eval()
    start = 0
    end = 0
    pred_list = []
    batch_size = 16
    with torch.no_grad():
      while start < test_data_array.shape[0]:
        end = start + batch_size 
        if end >= test_data_array.shape[0]:
            batch_size = test_data_array.shape[0] - start

        data = te_dataloader.load_te_batch(batch_size=batch_size, nway=nway, num_shot=nshot)

        test_x = test_data_array[start:end]

        data[0] = torch.from_numpy(test_x).to(device)

        data_cuda = [_data.to(device) for _data in data]

        map_label2class = data[-1].cpu().numpy()

        logsoft_prob = self.model(data_cuda)
        
        pred = torch.argmax(logsoft_prob, dim=1).cpu().numpy()

        pred = map_label2class[range(len(pred)), pred]

        pred_list.append(pred)

        start = end

    return np.hstack(pred_list)

pretrain_path = './drive/MyDrive/CSI_data/model_fine_tuning.ckpt'

trainer = Trainer()
trainer.load_pretrain(pretrain_path)
trainer.train()